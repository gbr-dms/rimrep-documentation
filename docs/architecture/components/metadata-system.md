# Metadata system

The metadata system serves as a comprehensive framework for organizing and managing datasets, facilitating efficient data discovery. The system follows the [STAC (SpatioTemporal Asset Catalogs) specification](https://stacspec.org/en) which provides a standardized way to expose collections of spatial temporal data. The metadata system comprises three core components: the API back-end [`stac-fastapi`](https://stac.reefdata.io/), the API front-end [`stac-browser`](https://stac.reefdata.io/browser), and the metadata catalog point-of-truth repository [`rimrep-catalog`](https://github.com/aodn/rimrep-catalog). The API back-end `stac-fastapi` enables dataset discovery via a REST API, while the front-end, `stac-browser`, offers a web-based interface for users to search datasets using various filters such as dataset name, data provider, keywords, date range, and spatial extents. The metadata catalog repository, `rimrep-catalog`, serves as a version-controlled repository to store all the metadata catalog files, including `frictionless` metadata files, accessed by automated pipelines to update the data and metadata in the DMS. Additionally, a [metadata entry tool](https://met.reefdata.io/) is provided as a temporary solution to create metadata records for datasets lacking existing metadata records.

## Summary

- Authentication: [`Keycloak`](https://www.keycloak.org/) (See [auth architecture](auth.md))
- Authorization: [`KrakenD`](https://www.krakend.io/) (See [auth architecture](auth.md))
- Metadata API back-end: [`rimrep-stac-fastapi`](https://github.com/aodn/rimrep-stac-fastapi)
  - [`pgstac`](https://github.com/stac-utils/pgstac) back-end - with AWS RDS PostgreSQL instance
- Metadata API front-end: [`rimrep-stac-browser`](https://github.com/aodn/rimrep-stac-browser)
- Metadata entry tool: [`rimrep-metcalf`](https://github.com/aodn/rimrep-metcalf)
  - Temporary tool to create metadata records to be ingested into external metadata management systems
  - Not integrated into any other components
- Metadata catalog point-of-truth: [`rimrep-catalog`](https://github.com/aodn/rimrep-catalog)
  - Version controlled - GitHub repository
  - Stores the templates, specification documents and human-curated metadata files for the automated metadata pipelines
    - Manually curated metadata files for STAC Collections (collection.jsonnet)
    - Manually curated metadata files for STAC Items using [`frictionless`](https://specs.frictionlessdata.io/) framework (datapackage.json, tableschema.json, gridschema.json)
    - Jsonnet library files to help with metadata manipulation in the automated metadata pipelines
    - Documentation for the STAC specification and guidelines we use
    - Files containing lists of datasets for metadata harvester to harvest
  - Automated publishing to [`rimrep-stac-fastapi`](https://github.com/aodn/rimrep-stac-fastapi)
- Complete [`frictionless`](https://specs.frictionlessdata.io/) metadata files generated by automated metadata pipelines are stored on S3 together with the data for users to access.
- Metadata harvester: metadata harvester comprises a series of Argo Workflows that need be triggered manually or run automatically to simplify the metadata harvesting process.

## Architecture

_(Note: Dashed line = not implemented yet)_

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'edgeLabelBackground': '#ffffff',
      'tertiaryTextColor': '#0f00aa',
      'clusterBkg': '#fafaff',
      'clusterBorder': '#0f00aa'
    }
  }
}%%

flowchart TB
  classDef red fill:#ffcccc,stroke:#ff0000;

  subgraph data_providers ["Data providers"]
    external((Data Providers <br> with metadata records))
    metadata_providers((Data Providers <br> without metadata records))
  end

  subgraph github ["GitHub"]
    catalog(rimrep-catalog)
  end

  subgraph aws_rds["AWS RDS Postgres"]
      aws_rds_metcalf[(metcalf DB)]
      aws_rds_stac[(stac DB)]
      aws_rds_metcalf ~~~ aws_rds_stac
  end

  subgraph "k8s"

    subgraph stac_fastapi_group["Metadata back-end"]
      stac_fastapi(stac-fastapi)
      internal_stac_fastapi(stac-fastapi-internal)
    end

    subgraph frontend_group["Metadata front-end"]
      metadata_frontend(stac-browser)
    end

    metadata_harvester(metadata-harvester)
    metadata_pipeline(metadata-pipeline)
    krakend(KrakenD)
    keycloak(Keycloak)

    metcalf(metcalf)
  end

  data_providers ~~~ aws_rds

  external_users((External Users))

  internal_stac_fastapi -->  |Write STAC JSON to|aws_rds_stac
  metcalf --> |Store metadata records in| aws_rds_metcalf
  aws_rds_stac --> |Read STAC JSON|stac_fastapi

  external --> metadata_harvester
  metadata_harvester --> catalog
  metcalf -. Harvest .-> metadata_harvester

  catalog --"Curated metadata files"--> metadata_pipeline
  metadata_pipeline -- "Generated STAC JSON"--> internal_stac_fastapi
 
  stac_fastapi -->|STAC REST API| krakend & metadata_frontend
  metadata_frontend --> |Web UI| krakend
  krakend --> keycloak
  keycloak --> external_users

  metadata_providers -->|Create records in| metcalf
```

### Metadata flow 

_(Note: Dashed line = not implemented yet)_

```mermaid
%%{
  init: {
    'theme': 'base',
    'themeVariables': {
      'edgeLabelBackground': '#ffffff',
      'tertiaryTextColor': '#0f00aa',
      'clusterBkg': '#fafaff',
      'clusterBorder': '#0f00aa'
    }
  }
}%%

flowchart TB
  classDef red fill:#ffcccc,stroke:#ff0000;

  subgraph github["GitHub"]
    catalog(rimrep-catalog)
  end

  subgraph group_external_metadata["External Metadata"]
    rks(Reef Knowledge System)
    external_metadata(Other external metadata)
  end

  subgraph group_external_data["External Data"]
    external_data_api[(API)]
    external_data_storage[(Storage)]
  end

  subgraph workflows["Argo Workflows"]
    metadata_harvester("metadata-harvester\n(creator harvester)")

    subgraph pipeline["Dataset pipeline"]
      direction TB
      harvest["harvest metadata\n(updater harvester)"]
      transform[transform data & metadata]
      ingest[ingest]
      upload[upload]
    end

  end

  subgraph AWS["AWS"]
    s3[(S3 bucket)]
    stac_db[(STAC DB)]
  end

  metadata_providers((Data Providers))
  metcalf(metcalf)
  rimrep_admin((DMS Admin))
  stac_fastapi_internal(stac-fastapi-internal)

  group_external_data ~~~ workflows

  metadata_providers -->|Manually create records using| metcalf
  metcalf  -. Ingested to .-> rks
  rks -. Harvest .-> metadata_harvester
  external_metadata --> |Harvest external metadata| metadata_harvester
  metadata_harvester --> |Harvest metadata\n & create dataset issues|catalog
  group_external_data -->|Fetch data| transform
  rimrep_admin -->|"Curate dataset issues"| catalog
  harvest --> |Updated datapackage.json|catalog
  harvest --> |datapackage.json|transform --> ingest & upload
  catalog --> |"tableschema.json/gridschema.json\ncollection.jsonnet\n*.libsonnet"| transform
  ingest -->  |"STAC Collections & Items"|stac_fastapi_internal
  stac_fastapi_internal --> |Writes to| stac_db
  upload --> |"Zarr/Parquet data"|s3
  upload --> |"Frictionless datapackage.json &\ntableschema.json/gridschema.json"| s3
```

### Metadata pipeline

Flowchart of the detailed steps in the metadata pipeline.

_(The detailed steps for metadata harvester is illustrated further in the [metadata harvester flowchart](#metadata-harvester-flowchart))_

```mermaid

flowchart TB

    A[generate-datadriven-metadata] --> N{File format}

    N --> |Parquet| O[generate-datadriven-metadata-parquet]

    N --> |Zarr| P[generate-datadriven-metadata-zarr]

    O --> C[generate-complete-datapackagejson]

    P --> C

    B[get-catalog-files] --> C

    C --> D[upload-complete-datapackagejson]

    C --> E[generate-stac-item]

    B --> E

    E --> F[validate-stac-item]

    F --> G{check-stac-collection-exist}

    G -->|No| H[create-stac-collection]

    H --> I[ingest-initial-stac-collection]

    I --> J[ingest-stac-item]

    G -->|Yes| J

    J --> K[generate-updated-collection-from-items]

    K --> L[validate-updated-collection]

    L --> M[ingest-updated-stac-collection]  
    Z((Metadata Harvester)) --> B

```

### Metadata harvester flowchart

_(Note: red color means the step is manual)_

```mermaid
flowchart TD
  classDef red fill:#ffcccc,stroke:#ff0000;
  classDef green fill:#bbf7d0,stroke:#16a34a;

    rimrep_catalog[rimrep-catalog] 

    subgraph metadata_harvester["Metadata Harvester"]
        harvest_metadata_create[harvest-metadata]
        harvest_metadata_update[harvest-metadata]
        read_data_to_create[read-list-of-datasets-to-harvest]
        create_github_ticket[create-github-dataset-tickets]
        read_data_to_update[read-harvestlink-with-github-issueno]
        compare_metadata{compared-metadata-for-change}
        regenerate_datapackagejson[regenerate-datapackagejson]
        submit_pr_auto[submit-datapackagejson-pr]
        update_dataset_ticket[update-dataset-ticket-with-harvested-fields]
        notify_stakeholders[notify-DMS-metadata-personnel]
        generate_datapackagejson[generate-datapackagejsons]
        human_intervention[manual-update/complete-dataset-tickets]:::red
        submit_pr_manual[submit-datapackagejsons-pr]
        trigger_manual([manual-trigger-first-harvest]):::red
        trigger_auto([auto-trigger-update-by-datawatcher-or-cronjob])

        trigger_manual --> read_data_to_create
        trigger_auto --> read_data_to_update
        read_data_to_create --> harvest_metadata_create
        read_data_to_update --> harvest_metadata_update
        harvest_metadata_update --> compare_metadata
        compare_metadata --> |Changed| update_dataset_ticket
        compare_metadata --> |No change| exit([exit])
        update_dataset_ticket --> regenerate_datapackagejson
        regenerate_datapackagejson --> submit_pr_auto
        harvest_metadata_create --> create_github_ticket
        create_github_ticket --> notify_stakeholders
        notify_stakeholders --> human_intervention
        human_intervention --> generate_datapackagejson
        generate_datapackagejson --> submit_pr_manual

    end
    
    submit_pr_manual --> rimrep_catalog
    submit_pr_auto --> rimrep_catalog
    
    
```

### Metadata API back-end

We are using [`rimrep-stac-fastapi`](https://github.com/aodn/rimrep-stac-fastapi) (a fork of [`stac-fastapi`](https://github.com/stac-utils/stac-fastapi)) to publish STAC API. It is using the [`pgstac`](https://github.com/stac-utils/pgstac) back-end with an AWS RDS PostgreSQL instance.

Note: there are two deployments, one is called `stac-fastapi-internal`, it has read/write access and is only accessible from within the k8s cluster. The other is called `stac-fastapi`, it only has read access and is accessible from outside the k8s cluster.

### Metadata API front-end

We are using [`rimrep-stac-browser`](https://github.com/aodn/rimrep-stac-browser) (a fork of [`stac-browser`](https://github.com/radiantearth/stac-browser)), which provides a simple Web UI to browse STAC API.

### Metadata entry tool

We are using [`rimrep-metcalf`](https://github.com/aodn/rimrep-metcalf) to provide a Web UI for data providers to use to create metadata records.

This component is temporary, it is only used by data providers that don't have metadata records in an external metadata management system. The goal is to ingest all created metadata records into an external metadata management system by the end of this phase of the project.

We will temporarily use records created by metcalf until they have been ingested into an external metadata management system.

### Metadata catalog point-of-truth

GitHub repository - https://github.com/aodn/rimrep-catalog - using [`jsonnet`](https://jsonnet.org/) JSON template language.

The publishing of STAC Collections and Items to `stac-fastapi-internal` has been automated in the metadata pipeline.

### Public Frictionless metadata files

Published as [Frictionless Datapackage](https://specs.frictionlessdata.io/data-package/) json files in the same folder as the data. The basic datapackage structure has been extended and specialised for the GBR DMS and a [Profile](https://specs.frictionlessdata.io/profiles/) describing it (as a json schema) is available on github [`rimrep-catalog`](https://github.com/aodn/rimrep-catalog/blob/main/templates/gbr-dms-data-package.json) for version control and at `s3://gbr-dms-files-public/gbr-dms-data-package.json` for public access.

Datapackage files are generated by ingestion workflows as a combination of human-curated metadata from [`rimrep-catalog`](https://github.com/aodn/rimrep-catalog) and data-driven metadata extracted from the data itself by the scripts in [`rimrep-data-pipeline`](https://github.com/aodn/rimrep-data-pipeline/tree/main/reefdata_stac/reefdata_stac). 

Tabular datasets are also associated with a [Tableschema](https://specs.frictionlessdata.io/table-schema/) file listing the names and types of all columns in the dataset. Gridded datasets use a `Gridschema` file instead, which has been designed on purpose for this system following the specifications of both [Frictionless schema](https://specs.frictionlessdata.io/data-resource/#resource-schemas) and [Stac datacube](https://github.com/stac-extensions/datacube).

The generated datapackage files act as a central repository of all known information about each dataset, and STAC items and pygeoapi configuration entries are generated programmatically based exclusively on these files.

### Metadata harvester

For new datasets that exist in external metadata catalogs, we manually trigger the metadata harvester to harvest them in bulk. The harvester will collect the metadata and create dataset issues in the `rimrep-catalog` GitHub repository to capture the harvested information. These dataset tickets need to be reviewed by the metadata personnel in the team to double-check the information and fill in any missing data. Following this, an Argo Workflow template named `bulk-generate-datapkg-and-submit` can be called to generate the `datapackage.json` files in bulk for the new dataset issues and submit them to `rimrep-catalog`.

For existing datasets in the DMS, the metadata harvester runs automatically (either attached to a data-watcher or as a cronjob) to periodically check whether the original metadata has been updated. If an update of the fields that we are interested in has been detected, the metadata harvester will collect the new data and update the GitHub dataset issue. Subsequently, it will automatically regenerate the `datapackage.json` file and submit to `rimrep-catalog` for review.  

## Auth

See [authentication](auth.md) component documentation.

Currently, the `stac-fastapi` and `stac-browser` requires authentication for all STAC collections/items. Authorization is required for limited access STAC collections/items through `KrakenD`.
